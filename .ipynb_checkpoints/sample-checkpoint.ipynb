{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31370d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc1 = SVC(kernel='sigmoid', gamma=1.0)\n",
    "knc1 = KNeighborsClassifier()\n",
    "mnb1 = MultinomialNB()\n",
    "dtc1 = DecisionTreeClassifier(max_depth=3)\n",
    "lrc1 = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "rfc1 = RandomForestClassifier(n_estimators=50, random_state=2)\n",
    "abc1 = AdaBoostClassifier(n_estimators=50, random_state=2)\n",
    "bc1 = BaggingClassifier(n_estimators=50, random_state=2)\n",
    "etc1 = ExtraTreesClassifier(n_estimators=50, random_state=2)\n",
    "gbdt1 = GradientBoostingClassifier(n_estimators=50,random_state=2)\n",
    "xgb1 = XGBClassifier(n_estimators=50,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987f40fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc2 = SVC(kernel='sigmoid', gamma=1.0)\n",
    "knc2 = KNeighborsClassifier()\n",
    "mnb2 = MultinomialNB()\n",
    "dtc2 = DecisionTreeClassifier(max_depth=3)\n",
    "lrc2 = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "rfc2 = RandomForestClassifier(n_estimators=50, random_state=2)\n",
    "abc2 = AdaBoostClassifier(n_estimators=50, random_state=2)\n",
    "bc2 = BaggingClassifier(n_estimators=50, random_state=2)\n",
    "etc2 = ExtraTreesClassifier(n_estimators=50, random_state=2)\n",
    "gbdt2 = GradientBoostingClassifier(n_estimators=50,random_state=2)\n",
    "xgb2 = XGBClassifier(n_estimators=50,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ee01e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs1 = {\n",
    "    'SVC' : svc1,\n",
    "    'KN' : knc1, \n",
    "    'NB': mnb1, \n",
    "    'DT': dtc1, \n",
    "    'LR': lrc1, \n",
    "    'RF': rfc1, \n",
    "    'AdaBoost': abc1, \n",
    "    'BgC': bc1, \n",
    "    'ETC': etc1,\n",
    "    'GBDT':gbdt1,\n",
    "    'xgb':xgb1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b952ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs2 = {\n",
    "    'SVC' : svc2,\n",
    "    'KN' : knc2, \n",
    "    'NB': mnb2, \n",
    "    'DT': dtc2, \n",
    "    'LR': lrc2, \n",
    "    'RF': rfc2, \n",
    "    'AdaBoost': abc2, \n",
    "    'BgC': bc2, \n",
    "    'ETC': etc2,\n",
    "    'GBDT':gbdt2,\n",
    "    'xgb':xgb2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6cf37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(clf,X_train,y_train,X_test,y_test):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    \n",
    "    return accuracy,precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83da0841",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores_tfidf = []\n",
    "precision_scores_tfidf = []\n",
    "\n",
    "for name,clf in clfs1.items():\n",
    "    \n",
    "    current_accuracy_tfidf,current_precision_tfidf = train_classifier(clf, X_train_tfidf,y_train,X_test_tfidf,y_test)\n",
    "    \n",
    "    print(\"For \",name)\n",
    "    print(\"Accuracy - \",current_accuracy_tfidf)\n",
    "    print(\"Precision - \",current_precision_tfidf)\n",
    "    \n",
    "    accuracy_scores_tfidf.append(current_accuracy_tfidf)\n",
    "    precision_scores_tfidf.append(current_precision_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01efe0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(tfidf,open('tfidf_vectorizer.pkl','wb'))\n",
    "pickle.dump(cv,open('bow_vectorizer.pkl','wb'))\n",
    "pickle.dump(mnb1,open('mnb_model.pkl','wb'))\n",
    "pickle.dump(gnb1,open('gnb_model.pkl','wb'))\n",
    "pickle.dump(bnb1,open('bnb_model.pkl','wb'))\n",
    "pickle.dump(svc1,open('svc_model.pkl','wb'))\n",
    "#pickle.dump(knc1,open('knc_model.pkl','wb'))\n",
    "#pickle.dump(dtc1,open('dtc_model.pkl','wb'))\n",
    "#pickle.dump(lrc1,open('lrc_model.pkl','wb'))\n",
    "#pickle.dump(rfc1,open('rfc_model.pkl','wb'))\n",
    "#pickle.dump(abc1,open('abc_model.pkl','wb'))\n",
    "#pickle.dump(bc1,open('bc_model.pkl','wb'))\n",
    "#pickle.dump(gbdt1,open('gbdt_model.pkl','wb'))\n",
    "#pickle.dump(xgb1,open('xgb_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478d1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores_bow = []\n",
    "precision_scores_bow = []\n",
    "\n",
    "for name,clf in clfs2.items():\n",
    "    \n",
    "    current_accuracy_bow,current_precision_bow = train_classifier(clf, X_train_bow,y_train,X_test_bow,y_test)\n",
    "    \n",
    "    print(\"For \",name)\n",
    "    print(\"Accuracy - \",current_accuracy_bow)\n",
    "    print(\"Precision - \",current_precision_bow)\n",
    "    \n",
    "    accuracy_scores_bow.append(current_accuracy_bow)\n",
    "    precision_scores_bow.append(current_precision_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#pickle.dump(tfidf,open('tfidf_vectorizer.pkl','wb'))\n",
    "#pickle.dump(cv,open('bow_vectorizer.pkl','wb'))\n",
    "pickle.dump(mnb2,open('mnb2_model.pkl','wb'))\n",
    "pickle.dump(gnb2,open('gnb2_model.pkl','wb'))\n",
    "pickle.dump(bnb2,open('bnb2_model.pkl','wb'))\n",
    "pickle.dump(svc2,open('svc2_model.pkl','wb'))\n",
    "#pickle.dump(knc2,open('knc2_model.pkl','wb'))\n",
    "#pickle.dump(dtc2,open('dtc2_model.pkl','wb'))\n",
    "#pickle.dump(lrc2,open('lrc2_model.pkl','wb'))\n",
    "#pickle.dump(rfc2,open('rfc2_model.pkl','wb'))\n",
    "#pickle.dump(abc2,open('abc2_model.pkl','wb'))\n",
    "#pickle.dump(bc2,open('bc2_model.pkl','wb'))\n",
    "#pickle.dump(gbdt2,open('gbdt2_model.pkl','wb'))\n",
    "#pickle.dump(xgb2,open('xgb2_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc584a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the saved TF-IDF vectorizer and MNB model\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    tfidf_vectorizer = pickle.load(f)\n",
    "\n",
    "with open('bow_vectorizer.pkl', 'rb') as f:\n",
    "    bow_vectorizer = pickle.load(f)\n",
    "    \n",
    "with open('mnb_model.pkl', 'rb') as f:\n",
    "    mnb_model = pickle.load(f)\n",
    "    \n",
    "with open('gnb_model.pkl', 'rb') as f:\n",
    "    gnb_model = pickle.load(f)\n",
    "    \n",
    "with open('bnb_model.pkl', 'rb') as f:\n",
    "    bnb_model = pickle.load(f)\n",
    "\n",
    "with open('svc_model.pkl', 'rb') as f:\n",
    "    svc_model = pickle.load(f)\n",
    "    \n",
    "with open('svc_model.pkl', 'rb') as f:\n",
    "    knc_model = pickle.load(f)\n",
    "    \n",
    "with open('dtc_model.pkl', 'rb') as f:\n",
    "    dtc_model = pickle.load(f)\n",
    "    \n",
    "with open('lrc_model.pkl', 'rb') as f:\n",
    "    lrc_model = pickle.load(f)\n",
    "    \n",
    "with open('rfc_model.pkl', 'rb') as f:\n",
    "    rfc_model = pickle.load(f)\n",
    "\n",
    "with open('abc_model.pkl', 'rb') as f:\n",
    "    abc_model = pickle.load(f)\n",
    "    \n",
    "with open('bc_model.pkl', 'rb') as f:\n",
    "    bc_model = pickle.load(f)\n",
    "    \n",
    "with open('xgb_model.pkl', 'rb') as f:\n",
    "    xgb_model = pickle.load(f)\n",
    "    \n",
    "with open('gbdt_model.pkl', 'rb') as f:\n",
    "    gbdt_model = pickle.load(f)\n",
    "    \n",
    "    \n",
    "\n",
    "new_inputs = 'bhatte bohot harami xa'\n",
    "\n",
    "preprocessed_inputs = clean_sent(new_inputs)\n",
    "\n",
    "preprocessed_inputs = [preprocessed_inputs]\n",
    "# Transform the preprocessed inputs using TF-IDF vectorization\n",
    "tfidf_features = tfidf_vectorizer.transform(preprocessed_inputs)\n",
    "tfidf_features1 = tfidf_vectorizer.transform(preprocessed_inputs).toarray()\n",
    "\n",
    "# Predict the results\n",
    "prediction_t_mnb = mnb_model.predict(tfidf_features)\n",
    "prediction_t_gnb = gnb_model.predict(tfidf_features1)\n",
    "prediction_t_bnb = bnb_model.predict(tfidf_features)\n",
    "prediction_t_svc = svc_model.predict(tfidf_features1) \n",
    "prediction_t_knc = knc_model.predict(tfidf_features1) \n",
    "prediction_t_dtc = dtc_model.predict(tfidf_features) \n",
    "prediction_t_lrc = lrc_model.predict(tfidf_features) \n",
    "prediction_t_rfc = rfc_model.predict(tfidf_features) \n",
    "prediction_t_abc = abc_model.predict(tfidf_features)\n",
    "prediction_t_bc = bc_model.predict(tfidf_features)\n",
    "prediction_t_xgb = xgb_model.predict(tfidf_features)\n",
    "prediction_t_gbdt = gbdt_model.predict(tfidf_features)\n",
    "\n",
    "print('prediction from mnb_model : ' ,prediction_t_mnb )\n",
    "print('prediction from gnb_model : ' ,prediction_t_gnb )\n",
    "print('prediction from bnb_model : ' ,prediction_t_bnb )\n",
    "print('prediction from svc_model : ' ,prediction_t_svc )\n",
    "print('prediction from knc_model : ' ,prediction_t_knc )\n",
    "print('prediction from dtc_model : ' ,prediction_t_dtc )\n",
    "print('prediction from lrc_model : ' ,prediction_t_lrc )\n",
    "print('prediction from rfc_model : ' ,prediction_t_rfc )\n",
    "print('prediction from abc_model : ' ,prediction_t_abc )\n",
    "print('prediction from bc_model : ' ,prediction_t_bc )\n",
    "print('prediction from xgb_model : ' ,prediction_t_xgb )\n",
    "print('prediction from gbdt_model : ' ,prediction_t_gbdt )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b725514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bow_vectorizer.pkl', 'rb') as f:\n",
    "    bow_vectorizer = pickle.load(f)\n",
    "    \n",
    "with open('mnb2_model.pkl', 'rb') as f:\n",
    "    mnb2_model = pickle.load(f)\n",
    "    \n",
    "with open('gnb2_model.pkl', 'rb') as f:\n",
    "    gnb2_model = pickle.load(f)\n",
    "    \n",
    "with open('bnb2_model.pkl', 'rb') as f:\n",
    "    bnb2_model = pickle.load(f)\n",
    "\n",
    "with open('svc2_model.pkl', 'rb') as f:\n",
    "    svc2_model = pickle.load(f)\n",
    "    \n",
    "with open('svc2_model.pkl', 'rb') as f:\n",
    "    knc2_model = pickle.load(f)\n",
    "    \n",
    "with open('dtc2_model.pkl', 'rb') as f:\n",
    "    dtc2_model = pickle.load(f)\n",
    "    \n",
    "with open('lrc2_model.pkl', 'rb') as f:\n",
    "    lrc2_model = pickle.load(f)\n",
    "    \n",
    "with open('rfc2_model.pkl', 'rb') as f:\n",
    "    rfc2_model = pickle.load(f)\n",
    "\n",
    "with open('abc2_model.pkl', 'rb') as f:\n",
    "    abc2_model = pickle.load(f)\n",
    "    \n",
    "with open('bc2_model.pkl', 'rb') as f:\n",
    "    bc2_model = pickle.load(f)\n",
    "    \n",
    "with open('xgb2_model.pkl', 'rb') as f:\n",
    "    xgb2_model = pickle.load(f)\n",
    "    \n",
    "with open('gbdt2_model.pkl', 'rb') as f:\n",
    "    gbdt2_model = pickle.load(f)\n",
    "    \n",
    "    \n",
    "\n",
    "new_inputs = 'meor tw vayana kaam aaja'\n",
    "\n",
    "preprocessed_inputs = clean_sent(new_inputs)\n",
    "\n",
    "preprocessed_inputs = [preprocessed_inputs]\n",
    "# Transform the preprocessed inputs using TF-IDF vectorization\n",
    "bow_features = bow_vectorizer.transform(preprocessed_inputs)\n",
    "bow_features1 = bow_vectorizer.transform(preprocessed_inputs).toarray()\n",
    "\n",
    "# Predict the results\n",
    "prediction_b_mnb = mnb2_model.predict(bow_features)\n",
    "prediction_b_gnb = gnb2_model.predict(bow_features1)\n",
    "prediction_b_bnb = bnb2_model.predict(bow_features)\n",
    "prediction_b_svc = svc2_model.predict(bow_features1) \n",
    "prediction_b_knc = knc2_model.predict(bow_features1) \n",
    "prediction_b_dtc = dtc2_model.predict(bow_features) \n",
    "prediction_b_lrc = lrc2_model.predict(bow_features) \n",
    "prediction_b_rfc = rfc2_model.predict(bow_features) \n",
    "prediction_b_abc = abc2_model.predict(bow_features)\n",
    "prediction_b_bc = bc2_model.predict(bow_features)\n",
    "prediction_b_xgb = xgb2_model.predict(bow_features)\n",
    "prediction_b_gbdt = gbdt2_model.predict(bow_features)\n",
    "\n",
    "print('prediction from mnb_model : ' ,prediction_b_mnb )\n",
    "print('prediction from gnb_model : ' ,prediction_b_gnb )\n",
    "print('prediction from bnb_model : ' ,prediction_b_bnb )\n",
    "print('prediction from svc_model : ' ,prediction_b_svc )\n",
    "print('prediction from knc_model : ' ,prediction_b_knc )\n",
    "print('prediction from dtc_model : ' ,prediction_b_dtc )\n",
    "print('prediction from lrc_model : ' ,prediction_b_lrc )\n",
    "print('prediction from rfc_model : ' ,prediction_b_rfc )\n",
    "print('prediction from abc_model : ' ,prediction_b_abc )\n",
    "print('prediction from bc_model : ' ,prediction_b_bc )\n",
    "print('prediction from xgb_model : ' ,prediction_b_xgb )\n",
    "print('prediction from gbdt_model : ' ,prediction_b_gbdt )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5619c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictions_t = np.round(np.mean([prediction_t_mnb,prediction_t_gnb,prediction_t_bnb,prediction_t_svc,prediction_t_knc,prediction_t_dtc,prediction_t_lrc,prediction_t_rfc,prediction_t_abc,prediction_t_bc,prediction_t_xgb,prediction_t_gbdt], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5aa196",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictions_b = np.round(np.mean([prediction_b_mnb,prediction_b_gnb,prediction_b_bnb,prediction_b_svc,prediction_b_knc,prediction_b_dtc,prediction_b_lrc,prediction_b_rfc,prediction_b_abc,prediction_b_bc,prediction_b_xgb,prediction_b_gbdt], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee94d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
